{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:09:20.536415Z",
     "iopub.status.busy": "2025-05-05T23:09:20.536131Z",
     "iopub.status.idle": "2025-05-05T23:10:31.500363Z",
     "shell.execute_reply": "2025-05-05T23:10:31.499267Z",
     "shell.execute_reply.started": "2025-05-05T23:09:20.536386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechbrain in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: hyperpyyaml in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (1.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from speechbrain) (24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (1.15.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (2.5.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (2.5.1+cu118)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (4.67.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from speechbrain) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.9->speechbrain) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.9->speechbrain) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.9->speechbrain) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.9->speechbrain) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.9->speechbrain) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.9->speechbrain) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub->speechbrain) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->speechbrain) (0.4.6)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperpyyaml->speechbrain) (0.18.10)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.9->speechbrain) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\tejas\\appdata\\local\\temp\\pip-req-build-k1v5b7nd\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit fe29b8c487dfe554ee44e2a359605f82651e9095\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.52.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.52.0.dev0) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers==4.52.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.52.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.52.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.52.0.dev0) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\tejas\\AppData\\Local\\Temp\\pip-req-build-k1v5b7nd'\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (2.5.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install speechbrain\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7a53abba424ec6b12deb43db6df6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d3b1c91cf94f18bffec71e850350e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/50.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd8a1c9185f4a2da717a07c0ca67804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502efe62a77847318bfc7fb9a89b8c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cmu-arctic-xvectors.py:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6868d6d0e23464f9af69041c06014c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/21.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a659a3e6484468588e5962c2fdd31d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    sampling_rate = 16000\n",
    "\n",
    "# Load models with safetensors (if available)\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\", use_safetensors=True).to(Config.device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\", use_safetensors=True)\n",
    "\n",
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[0][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "# Inference function\n",
    "def infer(text, emotion, file_name):\n",
    "    combined_input = f\"{text} [EMOTION] {emotion}\"\n",
    "    inputs = processor(text=combined_input, return_tensors=\"pt\").to(Config.device)\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings.to(Config.device), vocoder=vocoder)\n",
    "    sf.write(f\"{file_name}.wav\", speech.cpu().numpy(), samplerate=Config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:19:58.048281Z",
     "iopub.status.busy": "2025-05-05T23:19:58.047693Z",
     "iopub.status.idle": "2025-05-05T23:19:58.123095Z",
     "shell.execute_reply": "2025-05-05T23:19:58.122217Z",
     "shell.execute_reply.started": "2025-05-05T23:19:58.048252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "C:\\Users\\tejas\\AppData\\Local\\Temp\\ipykernel_16784\\3761771796.py:15: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "from transformers import SpeechT5ForTextToSpeech\n",
    "from transformers import SpeechT5HifiGan\n",
    "from datasets import load_dataset,Audio\n",
    "import soundfile as sf\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from torch import manual_seed\n",
    "from datasets import Dataset\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:12:48.257812Z",
     "iopub.status.busy": "2025-05-05T23:12:48.257493Z",
     "iopub.status.idle": "2025-05-05T23:19:36.611667Z",
     "shell.execute_reply": "2025-05-05T23:19:36.610760Z",
     "shell.execute_reply.started": "2025-05-05T23:12:48.257786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch\n",
    "# !pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:19:48.081954Z",
     "iopub.status.busy": "2025-05-05T23:19:48.081459Z",
     "iopub.status.idle": "2025-05-05T23:19:48.088747Z",
     "shell.execute_reply": "2025-05-05T23:19:48.087580Z",
     "shell.execute_reply.started": "2025-05-05T23:19:48.081904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:20:07.143038Z",
     "iopub.status.busy": "2025-05-05T23:20:07.142697Z",
     "iopub.status.idle": "2025-05-05T23:20:07.149370Z",
     "shell.execute_reply": "2025-05-05T23:20:07.148453Z",
     "shell.execute_reply.started": "2025-05-05T23:20:07.142992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "manual_seed(32)\n",
    "class Config:\n",
    "    device='cuda'if torch.cuda.is_available() else 'cpu'\n",
    "    sampling_rate=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:20:17.556263Z",
     "iopub.status.busy": "2025-05-05T23:20:17.555779Z",
     "iopub.status.idle": "2025-05-05T23:20:18.191578Z",
     "shell.execute_reply": "2025-05-05T23:20:18.190222Z",
     "shell.execute_reply.started": "2025-05-05T23:20:17.556230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# model=model.to(Config.device)\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\") \n",
    "# embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "# speaker_embeddings = torch.tensor(embeddings_dataset[0][\"xvector\"]).unsqueeze(0) \n",
    "# def infer(text,emotion,file_name):\n",
    "#     combined_input = f\"{text} [EMOTION] {emotion}\"\n",
    "#     inputs = processor(text=combined_input, return_tensors=\"pt\")\n",
    "#     speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "#     sf.write(f\"{file_name}.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tejas\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tejas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchaudio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T23:11:40.627946Z",
     "iopub.status.busy": "2025-05-05T23:11:40.627070Z",
     "iopub.status.idle": "2025-05-05T23:12:41.378838Z",
     "shell.execute_reply": "2025-05-05T23:12:41.377540Z",
     "shell.execute_reply.started": "2025-05-05T23:11:40.627901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emo_abv={'fru': 'Frustration',\n",
    "'exc': 'Excitement',\n",
    "'neu': 'Neutral',\n",
    "'ang': 'Anger',\n",
    "'sad': 'Sadness',\n",
    "'hap': 'Happiness',\n",
    "'sur': 'Surprise',\n",
    "'fea': 'Fear',\n",
    "'oth': 'Other',\n",
    "'dis': 'Disgust'}\n",
    "\n",
    "path=\"./data/iemocapTrans.csv\"\n",
    "df=pd.read_csv(path)\n",
    "df.drop(['_id','translated'],axis=1,inplace=True)\n",
    "df['emotion']=df['emotion'].apply(lambda x: emo_abv[x])\n",
    "df.columns=['activation', 'dominance', 'emotion', 'end_time', 'start_time', 'audiofile_name','text', 'valence']\n",
    "\n",
    "audio_path=\"./data/Iemocap_audio/iemocap_audio/IEMOCAP_wav\"\n",
    "df['audio']=df['audiofile_name'].apply(lambda x: torchaudio.load(os.path.join(audio_path,x+\".wav\"))[0])\n",
    "df['text']=df['text']+\" [emotion] \"+df['emotion']\n",
    "\n",
    "df_=df[['text','audio']]\n",
    "dataset = Dataset.from_dict({'text':df_['text'],'target':df_['audio'].to_list()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8b9a86927449bf883e5e46e092a209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec893ca1455241c7817eec3baf5d8d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08873d0ab9f04b258d99d8e0afac9a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e577ca69142e8bf0cb0752f8209e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdf993908b44804ba60a20298c8e157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95e875db3324d46b51f7a8e6de0ac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name, \n",
    "    run_opts={\"device\": Config.device}, \n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name)\n",
    ")\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings\n",
    "\n",
    "def prepare_dataset(single_data):\n",
    "    example = processor(\n",
    "        text=single_data[\"text\"],\n",
    "        audio_target=single_data['target'], \n",
    "        sampling_rate=Config.sampling_rate,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(single_data['target'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64c9ba3be5a4baa879f89c7dd172082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids,\n",
    "            labels=label_features,\n",
    "            return_tensors=\"pt\")        \n",
    "\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n",
    "            target_lengths = target_lengths.new([length - length % model.config.reduction_factor for length in target_lengths])\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 162/4000 25:29 < 10:11:20, 0.10 it/s, Epoch 0.59/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 33\u001b[0m\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./out_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     label_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     22\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     31\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2239\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2237\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2548\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2550\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2552\u001b[0m )\n\u001b[0;32m   2553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2560\u001b[0m ):\n\u001b[0;32m   2561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2562\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3743\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3744\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3746\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3748\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3749\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3750\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3809\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3807\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3808\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3809\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m     name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map[\u001b[38;5;28mself\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[index, operator] # noqa: B950\u001b[39;00m\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[0;32m   1737\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py:2707\u001b[0m, in \u001b[0;36mSpeechT5ForTextToSpeech.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_values, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, speaker_embeddings, labels, stop_labels)\u001b[0m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2706\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m SpeechT5SpectrogramLoss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m-> 2707\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs_before_postnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs_after_postnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2713\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   2717\u001b[0m     output \u001b[38;5;241m=\u001b[39m (outputs_after_postnet,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m     name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map[\u001b[38;5;28mself\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[index, operator] # noqa: B950\u001b[39;00m\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[0;32m   1737\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tejas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py:1935\u001b[0m, in \u001b[0;36mSpeechT5SpectrogramLoss.forward\u001b[1;34m(self, attention_mask, outputs_before_postnet, outputs_after_postnet, logits, labels, cross_attentions)\u001b[0m\n\u001b[0;32m   1932\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100.0\u001b[39m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;66;03m# mask out the padded portions\u001b[39;00m\n\u001b[1;32m-> 1935\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1936\u001b[0m outputs_before_postnet \u001b[38;5;241m=\u001b[39m outputs_before_postnet\u001b[38;5;241m.\u001b[39mmasked_select(padding_mask)\n\u001b[0;32m   1937\u001b[0m outputs_after_postnet \u001b[38;5;241m=\u001b[39m outputs_after_postnet\u001b[38;5;241m.\u001b[39mmasked_select(padding_mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "a = Seq2SeqTrainingArguments()\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./out_dir\", \n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=1.0)\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "example = dataset[\"test\"][304]\n",
    "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n",
    "torch.save(speaker_embeddings,'speaker_embeddings.pt')\n",
    "\n",
    "def gen(prompt,emo,model,speaker_embeddings):\n",
    "    text=prompt+\" [emotion] \"+emo\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    spectrogram = model.generate_speech(inputs[\"input_ids\"].to('cpu'), speaker_embeddings.to('cpu'))\n",
    "    with torch.no_grad():\n",
    "            speech=vocoder(spectrogram)\n",
    "    return Audio(speech.cpu().numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating TTS with emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio=gen('where do you think you are going from here','Happiness',trainer.model.to('cpu'),torch.load('speaker_embeddings.pt'))\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "# from IPython.display import Audio\n",
    "\n",
    "# # Configuration\n",
    "# class Config:\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     sampling_rate = 16000\n",
    "\n",
    "# # Load processor and vocoder (these don't change)\n",
    "# processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\n",
    "#     \"microsoft/speecht5_hifigan\", \n",
    "#     use_safetensors=True\n",
    "# )\n",
    "\n",
    "# # Choose which checkpoint to load (select the highest number for best results)\n",
    "# checkpoint_path = \"./out_dir/checkpoint-4000\"  # or whichever checkpoint you prefer\n",
    "\n",
    "# # Load the fine-tuned model from checkpoint\n",
    "# model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint_path).to(Config.device)\n",
    "\n",
    "# # Load speaker embeddings (assuming they're saved in the out_dir)\n",
    "# speaker_embeddings = torch.load(\"./out_dir/speaker_embeddings.pt\").to(Config.device)\n",
    "\n",
    "# def generate_speech(prompt, emotion, model, speaker_embeddings):\n",
    "#     \"\"\"Generate speech from text using the fine-tuned model\"\"\"\n",
    "#     # Prepare input text with emotion tag\n",
    "#     text = f\"{prompt} [emotion] {emotion}\"\n",
    "    \n",
    "#     # Process text and generate spectrogram\n",
    "#     inputs = processor(text=text, return_tensors=\"pt\").to(Config.device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Generate speech\n",
    "#         spectrogram = model.generate_speech(\n",
    "#             inputs[\"input_ids\"], \n",
    "#             speaker_embeddings\n",
    "#         )\n",
    "        \n",
    "#         # Convert spectrogram to waveform\n",
    "#         speech = vocoder(spectrogram)\n",
    "    \n",
    "#     return speech.cpu().numpy()\n",
    "\n",
    "# # Example usage\n",
    "# prompt = \"where do you think you are going from here\"\n",
    "# emotion = \"Happiness\"  # Use one of your emotion labels\n",
    "\n",
    "# # Generate and play audio\n",
    "# audio = generate_speech(prompt, emotion, model, speaker_embeddings)\n",
    "# Audio(audio, rate=Config.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import SpeechT5Config, SpeechT5Processor, SpeechT5ForTextToSpeech\n",
    "# from transformers import SpeechT5HifiGan, SpeechT5HifiGanConfig\n",
    "# import os\n",
    "# from safetensors.torch import load_file\n",
    "\n",
    "# class Config:\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     sampling_rate = 16000\n",
    "\n",
    "# processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "# checkpoint_path = \"./out_dir/checkpoint-4000\"\n",
    "# model = SpeechT5ForTextToSpeech.from_pretrained(\n",
    "#     checkpoint_path,\n",
    "#     local_files_only=True,\n",
    "#     use_safetensors=True if os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")) else False\n",
    "# ).to(Config.device)\n",
    "\n",
    "# vocoder_config = SpeechT5HifiGanConfig.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "# vocoder = SpeechT5HifiGan(vocoder_config).to(Config.device)\n",
    "\n",
    "# try:\n",
    "#     vocoder_path = \"microsoft/speecht5_hifigan\"\n",
    "#     if os.path.exists(vocoder_path):\n",
    "#         vocoder_state_dict = torch.load(\n",
    "#             os.path.join(vocoder_path, \"pytorch_model.bin\"),\n",
    "#             map_location=\"cpu\",\n",
    "#             weights_only=True\n",
    "#         )\n",
    "#     else: \n",
    "#         from huggingface_hub import hf_hub_download\n",
    "#         vocoder_state_dict = torch.load(\n",
    "#             hf_hub_download(repo_id=\"microsoft/speecht5_hifigan\", filename=\"pytorch_model.bin\"),\n",
    "#             map_location=\"cpu\",\n",
    "#             weights_only=True\n",
    "#         )\n",
    "    \n",
    "#     vocoder.load_state_dict({\n",
    "#         k: v for k, v in vocoder_state_dict.items()\n",
    "#         if k in vocoder.state_dict()\n",
    "#     }, strict=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"Vocoder loading failed, trying alternative method: {e}\")\n",
    "#     vocoder = SpeechT5HifiGan.from_pretrained(\n",
    "#         \"microsoft/speecht5_hifigan\",\n",
    "#         use_safetensors=True,\n",
    "#         ignore_mismatched_sizes=True\n",
    "#     ).to(Config.device)\n",
    "\n",
    "# speaker_embeddings = torch.load(\n",
    "#     './out_dir/speaker_embeddings.pt',\n",
    "#     map_location=\"cpu\",\n",
    "#     weights_only=True\n",
    "# ).to(Config.device)\n",
    "\n",
    "# def generate_speech(prompt, emotion, max_length=500):\n",
    "#     try:\n",
    "#         text = f\"{prompt} [emotion] {emotion}\"\n",
    "#         inputs = processor(text=text, return_tensors=\"pt\").to(Config.device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             spectrogram = model.generate_speech(\n",
    "#                 inputs[\"input_ids\"],\n",
    "#                 speaker_embeddings,\n",
    "#                 max_length=max_length\n",
    "#             )\n",
    "#             speech = vocoder(spectrogram)\n",
    "#         return speech.cpu().numpy()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Generation failed: {e}\")\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from IPython.display import Audio\n",
    "# audio = generate_speech(\"This solution should now work perfectly\", \"Happiness\")\n",
    "# if audio is not None:\n",
    "#     display(Audio(audio, rate=Config.sampling_rate))\n",
    "# else:\n",
    "#     print(\"Audio generation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2108701,
     "sourceId": 3603507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30476,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
